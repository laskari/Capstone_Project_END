{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YKC3Nj0d8_VAP61joCkPYQjes0JrBsZe",
      "authorship_tag": "ABX9TyOni7Od9wTCd+GK+7mK39cP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71410ba390e149119f097c331db6040d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e4864f58c53425e95a600e0c9e1c1d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4fd61e8b9bff4e56bfe269b0689866e4",
              "IPY_MODEL_b84dacd84bb340dda34bd9577c71241f"
            ]
          }
        },
        "0e4864f58c53425e95a600e0c9e1c1d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fd61e8b9bff4e56bfe269b0689866e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1f05abe62524c9fb5a6f329ca8033cd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 12182,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 12182,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1ad4b72dcab4325a136726753c8c28c"
          }
        },
        "b84dacd84bb340dda34bd9577c71241f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_91b08ba338cd4aff8a11c30d3879fec5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 12182/12182 [06:56&lt;00:00, 29.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a696b0aad0f4f83aa320d29aac850cf"
          }
        },
        "d1f05abe62524c9fb5a6f329ca8033cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1ad4b72dcab4325a136726753c8c28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91b08ba338cd4aff8a11c30d3879fec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a696b0aad0f4f83aa320d29aac850cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laskari/Capstone_Project_END/blob/main/Capstone_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UqoGK7g3ckO",
        "outputId": "879ce1d9-65d3-416a-fe31-14ba8845d31c"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "\r\n",
        "file = open('/content/drive/MyDrive/END School of AI/Datasets/END Datasets/English_Python_processed.txt',\"rt\", encoding='latin')\r\n",
        "data_complete = file.read()\r\n",
        "print(data_complete[:500])\r\n",
        "print(len(data_complete))\r\n",
        "#print(data_complete[10000:30000])\r\n",
        "data_complete =  data_complete.lower()\r\n",
        "#print(\"\\nAfter Doing Lowercase\\n\")\r\n",
        "#print(data_complete[10000:30000])\r\n",
        "\r\n",
        "#data_complete = data_complete.replace(\"   \",\"    \")\r\n",
        "#data_complete = data_complete.replace(\"     \",\"    \")\r\n",
        "#data_complete = data_complete.replace(\" # \",\"#\")\r\n",
        "#data_complete = data_complete.replace(\"  # \",\"#\")\r\n",
        "#data_complete = data_complete.replace(\"#\",\"\\n#\")\r\n",
        "data_complete = re.sub(r'\"#[0123456789]','#', data_complete)\r\n",
        "data_complete = data_complete.replace(\"\\n\\n\",\"\\n\")\r\n",
        "data_complete = data_complete.replace(\"\\n\\n\\n\",\"\\n\\n\")\r\n",
        "\r\n",
        "#print(\"\\nAfter Removing Extra Line space\\n\")\r\n",
        "#print(data_complete[10000:30000])\r\n",
        "data_com = data_complete.split('#')\r\n",
        "#print(data_com[:5])\r\n",
        "\r\n",
        "qtn_text = []\r\n",
        "prog =[]\r\n",
        "\r\n",
        "for each_code in data_com:\r\n",
        "    pr_text = each_code.split('\\n')\r\n",
        "    qtn_text.append(pr_text[0])\r\n",
        "    prog.append(pr_text[1:])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# write a python program to add two numbers \n",
            "num1 = 1.5\n",
            "num2 = 6.3\n",
            "sum = num1 + num2\n",
            "print(f'Sum: {sum}')\n",
            "\n",
            "\n",
            "# write a python function to add two user provided numbers and return the sum\n",
            "def add_two_numbers(num1, num2):\n",
            "    sum = num1 + num2\n",
            "    return sum\n",
            "\n",
            "\n",
            "# write a program to find and print the largest among three numbers\n",
            "\n",
            "num1 = 10\n",
            "num2 = 12\n",
            "num3 = 14\n",
            "if (num1 >= num2) and (num1 >= num3):\n",
            "   largest = num1\n",
            "elif (num2 >= num1) and (num2 >= num3):\n",
            "   largest = num2\n",
            "else:\n",
            "   largest = num3\n",
            "print\n",
            "1120285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRQyobB24SJZ"
      },
      "source": [
        "df = pd.DataFrame({'Program_text': qtn_text, 'Program_code': prog})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vRK3Qcb4Yuk",
        "outputId": "df2cbfe0-2cdd-4641-ce5d-44a217e2d13c"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4650, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmC9T0Ld4nqE"
      },
      "source": [
        "import torch\r\n",
        "from torch.jit import script, trace\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import csv\r\n",
        "from torchtext.legacy import data\r\n",
        "from torchtext.legacy.data import Field, BucketIterator\r\n",
        "import random\r\n",
        "import re\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "import codecs\r\n",
        "from io import open\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DkxvTE14nuS"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm7FZCcD4ny4"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBfSnt254n2C",
        "outputId": "0056b689-3d6e-44ea-ca84-d28e139a541f"
      },
      "source": [
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCPvlqd_4n49"
      },
      "source": [
        "Program_code = Field(tokenize = 'spacy', \r\n",
        "            init_token = 'start', \r\n",
        "            eos_token = 'end', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)\r\n",
        "\r\n",
        "Program_text = Field(tokenize = 'spacy', \r\n",
        "            init_token = 'start', \r\n",
        "            eos_token = 'end', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XdOFtnO4n8w"
      },
      "source": [
        "fields = [('Program_text', Program_text),('Program_code',Program_code)]\r\n",
        "example = [data.Example.fromlist([df.Program_text[i],df.Program_code[i]], fields) for i in range(df.shape[0])] \r\n",
        "dataset = data.Dataset(example, fields)\r\n",
        "\r\n",
        "(train_data, valid_data, test_data) = dataset.split(split_ratio=[0.80, 0.10, 0.10], random_state=random.seed(SEED))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGkvAfqaiNYL",
        "outputId": "3d5d3d56-52a2-45d0-b0ca-687b2c16f4a7"
      },
      "source": [
        "print(vars(train_data.examples[2]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Program_text': [' ', 'write', 'a', 'program', 'that', 'adds', 'the', 'square', 'of', 'two', 'numbers', 'and', 'prints', 'it'], 'Program_code': ['a = 32', 'b = 21', 'result = a**2 + b**2', 'print(result)', '']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xb_0CLp4oAV"
      },
      "source": [
        "Program_text.build_vocab(train_data, min_freq = 1)\r\n",
        "Program_code.build_vocab(train_data, min_freq = 1)\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NArQAiWwE0Xh",
        "outputId": "520d211c-4838-466b-a934-e550fbadd5c7"
      },
      "source": [
        "print(\"Program_text Vocab size\", len(Program_text.vocab))\r\n",
        "print(\"Program_code Vocab size\", len(Program_code.vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Program_text Vocab size 2189\n",
            "Program_code Vocab size 12182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNSe7J4z4oDu"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data), \r\n",
        "     sort= False,\r\n",
        "     batch_size = BATCH_SIZE,\r\n",
        "     device = device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BQrX3utDyp2"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "def Tokenize(sentence):\r\n",
        "  sentence = str(sentence).replace('\\n', '\\t\\t')\r\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sentence)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKxhIOP3DjLG"
      },
      "source": [
        "import gensim\r\n",
        "w2v_dim = 256\r\n",
        "w2v_min_count = 2\r\n",
        "w2v_window = 3\r\n",
        "target = []\r\n",
        "for sent in df['Program_code'].values:\r\n",
        "  sent_token = Tokenize(sent)\r\n",
        "  target.append(sent_token)\r\n",
        "w2v_model = gensim.models.Word2Vec(target, size = w2v_dim, window = w2v_window, min_count = w2v_min_count)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "71410ba390e149119f097c331db6040d",
            "0e4864f58c53425e95a600e0c9e1c1d1",
            "4fd61e8b9bff4e56bfe269b0689866e4",
            "b84dacd84bb340dda34bd9577c71241f",
            "d1f05abe62524c9fb5a6f329ca8033cd",
            "d1ad4b72dcab4325a136726753c8c28c",
            "91b08ba338cd4aff8a11c30d3879fec5",
            "3a696b0aad0f4f83aa320d29aac850cf"
          ]
        },
        "id": "ZtnYrcI5FL5d",
        "outputId": "9c5d175a-bb3b-48c3-a9aa-9b3eadce2512"
      },
      "source": [
        "from tqdm import tqdm_notebook\r\n",
        "word2vec_vectors = []\r\n",
        "for token, idx in tqdm_notebook(Program_code.vocab.stoi.items()):\r\n",
        "  if token in w2v_model.wv.vocab.keys():\r\n",
        "    word2vec_vectors.append(torch.FloatTensor(w2v_model[token]))\r\n",
        "  else:\r\n",
        "    word2vec_vectors.append(torch.zeros(w2v_dim))\r\n",
        "Program_code.vocab.set_vectors(Program_code.vocab.stoi, word2vec_vectors, w2v_dim)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71410ba390e149119f097c331db6040d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=12182.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J-cW8reENFn"
      },
      "source": [
        "w2v_model.save('embeddings.txt')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQzDyY4K4oHM"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 250):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        #self.tok_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(Program_text.vocab.vectors))\r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "        \r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "            \r\n",
        "        return src"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpURbZPm4oLE"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5bD3V1R4oOl"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Z0FEnZ4oSB"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTEFKX_E4oVz"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 pre_trained_emb,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        #self.tok_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(Program_code.vocab.vectors))\r\n",
        "        self.pre_trained_emb = pre_trained_emb\r\n",
        "        self.tok_embedding = nn.Embedding.from_pretrained(self.pre_trained_emb)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "            \r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcD4nRwO4oZL"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU8WSknE4ocy"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwb_8IqY4ogn"
      },
      "source": [
        "INPUT_DIM = len(Program_text.vocab)\r\n",
        "OUTPUT_DIM = len(Program_code.vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1\r\n",
        "pre_trained_emb = torch.FloatTensor(Program_code.vocab.vectors)\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM,\r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT, \r\n",
        "              device, \r\n",
        "              pre_trained_emb)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI9Ydm2u4oj1"
      },
      "source": [
        "SRC_PAD_IDX = Program_text.vocab.stoi[Program_text.pad_token]\r\n",
        "TRG_PAD_IDX = Program_code.vocab.stoi[Program_code.pad_token]\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTJaXIm4onU"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)\r\n",
        "\r\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_717f0Gt4oqd"
      },
      "source": [
        "LEARNING_RATE = 0.0005\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02e3HQK4ot2"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n90PVBeh4oxk"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.Program_text\r\n",
        "        trg = batch.Program_code\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IL8dkKw4o0w"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.Program_text\r\n",
        "            trg = batch.Program_code\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNtgJ3wu7JRl"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgSJXge17JWC",
        "outputId": "a7501866-9923-43c2-94b4-7a0e5325457e"
      },
      "source": [
        "import time\r\n",
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 4s\n",
            "\tTrain Loss: 8.348 | Train PPL: 4223.035\n",
            "\t Val. Loss: 7.808 |  Val. PPL: 2459.850\n",
            "Epoch: 02 | Time: 0m 3s\n",
            "\tTrain Loss: 7.226 | Train PPL: 1375.097\n",
            "\t Val. Loss: 7.534 |  Val. PPL: 1870.887\n",
            "Epoch: 03 | Time: 0m 3s\n",
            "\tTrain Loss: 6.989 | Train PPL: 1084.543\n",
            "\t Val. Loss: 7.560 |  Val. PPL: 1919.728\n",
            "Epoch: 04 | Time: 0m 3s\n",
            "\tTrain Loss: 6.729 | Train PPL: 836.026\n",
            "\t Val. Loss: 7.424 |  Val. PPL: 1676.495\n",
            "Epoch: 05 | Time: 0m 3s\n",
            "\tTrain Loss: 6.355 | Train PPL: 575.224\n",
            "\t Val. Loss: 7.233 |  Val. PPL: 1384.192\n",
            "Epoch: 06 | Time: 0m 3s\n",
            "\tTrain Loss: 5.926 | Train PPL: 374.491\n",
            "\t Val. Loss: 6.959 |  Val. PPL: 1052.339\n",
            "Epoch: 07 | Time: 0m 3s\n",
            "\tTrain Loss: 5.593 | Train PPL: 268.536\n",
            "\t Val. Loss: 6.789 |  Val. PPL: 887.773\n",
            "Epoch: 08 | Time: 0m 3s\n",
            "\tTrain Loss: 5.204 | Train PPL: 181.998\n",
            "\t Val. Loss: 6.590 |  Val. PPL: 727.785\n",
            "Epoch: 09 | Time: 0m 3s\n",
            "\tTrain Loss: 4.826 | Train PPL: 124.736\n",
            "\t Val. Loss: 6.448 |  Val. PPL: 631.444\n",
            "Epoch: 10 | Time: 0m 3s\n",
            "\tTrain Loss: 4.509 | Train PPL:  90.843\n",
            "\t Val. Loss: 6.334 |  Val. PPL: 563.251\n",
            "Epoch: 11 | Time: 0m 3s\n",
            "\tTrain Loss: 4.202 | Train PPL:  66.822\n",
            "\t Val. Loss: 6.182 |  Val. PPL: 483.940\n",
            "Epoch: 12 | Time: 0m 3s\n",
            "\tTrain Loss: 3.951 | Train PPL:  51.997\n",
            "\t Val. Loss: 6.183 |  Val. PPL: 484.560\n",
            "Epoch: 13 | Time: 0m 3s\n",
            "\tTrain Loss: 3.630 | Train PPL:  37.701\n",
            "\t Val. Loss: 6.037 |  Val. PPL: 418.661\n",
            "Epoch: 14 | Time: 0m 3s\n",
            "\tTrain Loss: 3.367 | Train PPL:  28.993\n",
            "\t Val. Loss: 5.929 |  Val. PPL: 375.792\n",
            "Epoch: 15 | Time: 0m 3s\n",
            "\tTrain Loss: 3.121 | Train PPL:  22.678\n",
            "\t Val. Loss: 5.767 |  Val. PPL: 319.694\n",
            "Epoch: 16 | Time: 0m 3s\n",
            "\tTrain Loss: 2.885 | Train PPL:  17.901\n",
            "\t Val. Loss: 5.772 |  Val. PPL: 321.085\n",
            "Epoch: 17 | Time: 0m 3s\n",
            "\tTrain Loss: 2.649 | Train PPL:  14.144\n",
            "\t Val. Loss: 5.671 |  Val. PPL: 290.231\n",
            "Epoch: 18 | Time: 0m 3s\n",
            "\tTrain Loss: 2.405 | Train PPL:  11.083\n",
            "\t Val. Loss: 5.567 |  Val. PPL: 261.614\n",
            "Epoch: 19 | Time: 0m 3s\n",
            "\tTrain Loss: 2.216 | Train PPL:   9.172\n",
            "\t Val. Loss: 5.468 |  Val. PPL: 237.085\n",
            "Epoch: 20 | Time: 0m 3s\n",
            "\tTrain Loss: 2.020 | Train PPL:   7.542\n",
            "\t Val. Loss: 5.453 |  Val. PPL: 233.490\n",
            "Epoch: 21 | Time: 0m 3s\n",
            "\tTrain Loss: 1.818 | Train PPL:   6.160\n",
            "\t Val. Loss: 5.358 |  Val. PPL: 212.299\n",
            "Epoch: 22 | Time: 0m 3s\n",
            "\tTrain Loss: 1.644 | Train PPL:   5.178\n",
            "\t Val. Loss: 5.320 |  Val. PPL: 204.479\n",
            "Epoch: 23 | Time: 0m 3s\n",
            "\tTrain Loss: 1.477 | Train PPL:   4.382\n",
            "\t Val. Loss: 5.332 |  Val. PPL: 206.791\n",
            "Epoch: 24 | Time: 0m 3s\n",
            "\tTrain Loss: 1.295 | Train PPL:   3.650\n",
            "\t Val. Loss: 5.305 |  Val. PPL: 201.384\n",
            "Epoch: 25 | Time: 0m 3s\n",
            "\tTrain Loss: 1.147 | Train PPL:   3.149\n",
            "\t Val. Loss: 5.281 |  Val. PPL: 196.623\n",
            "Epoch: 26 | Time: 0m 3s\n",
            "\tTrain Loss: 1.013 | Train PPL:   2.754\n",
            "\t Val. Loss: 5.248 |  Val. PPL: 190.250\n",
            "Epoch: 27 | Time: 0m 3s\n",
            "\tTrain Loss: 0.902 | Train PPL:   2.464\n",
            "\t Val. Loss: 5.171 |  Val. PPL: 176.144\n",
            "Epoch: 28 | Time: 0m 3s\n",
            "\tTrain Loss: 0.786 | Train PPL:   2.194\n",
            "\t Val. Loss: 5.307 |  Val. PPL: 201.654\n",
            "Epoch: 29 | Time: 0m 3s\n",
            "\tTrain Loss: 0.673 | Train PPL:   1.961\n",
            "\t Val. Loss: 5.113 |  Val. PPL: 166.202\n",
            "Epoch: 30 | Time: 0m 3s\n",
            "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
            "\t Val. Loss: 5.165 |  Val. PPL: 175.050\n",
            "Epoch: 31 | Time: 0m 3s\n",
            "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
            "\t Val. Loss: 5.175 |  Val. PPL: 176.869\n",
            "Epoch: 32 | Time: 0m 3s\n",
            "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
            "\t Val. Loss: 5.230 |  Val. PPL: 186.861\n",
            "Epoch: 33 | Time: 0m 3s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
            "\t Val. Loss: 5.206 |  Val. PPL: 182.451\n",
            "Epoch: 34 | Time: 0m 3s\n",
            "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
            "\t Val. Loss: 5.227 |  Val. PPL: 186.206\n",
            "Epoch: 35 | Time: 0m 3s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
            "\t Val. Loss: 5.211 |  Val. PPL: 183.324\n",
            "Epoch: 36 | Time: 0m 3s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 5.241 |  Val. PPL: 188.840\n",
            "Epoch: 37 | Time: 0m 3s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 5.351 |  Val. PPL: 210.883\n",
            "Epoch: 38 | Time: 0m 3s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 5.395 |  Val. PPL: 220.367\n",
            "Epoch: 39 | Time: 0m 3s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 5.378 |  Val. PPL: 216.673\n",
            "Epoch: 40 | Time: 0m 3s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 5.366 |  Val. PPL: 214.020\n",
            "Epoch: 41 | Time: 0m 3s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
            "\t Val. Loss: 5.384 |  Val. PPL: 217.866\n",
            "Epoch: 42 | Time: 0m 3s\n",
            "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
            "\t Val. Loss: 5.459 |  Val. PPL: 234.829\n",
            "Epoch: 43 | Time: 0m 3s\n",
            "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
            "\t Val. Loss: 5.500 |  Val. PPL: 244.651\n",
            "Epoch: 44 | Time: 0m 3s\n",
            "\tTrain Loss: 0.131 | Train PPL:   1.141\n",
            "\t Val. Loss: 5.509 |  Val. PPL: 246.946\n",
            "Epoch: 45 | Time: 0m 3s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
            "\t Val. Loss: 5.546 |  Val. PPL: 256.254\n",
            "Epoch: 46 | Time: 0m 3s\n",
            "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
            "\t Val. Loss: 5.587 |  Val. PPL: 266.870\n",
            "Epoch: 47 | Time: 0m 3s\n",
            "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
            "\t Val. Loss: 5.524 |  Val. PPL: 250.720\n",
            "Epoch: 48 | Time: 0m 3s\n",
            "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
            "\t Val. Loss: 5.588 |  Val. PPL: 267.295\n",
            "Epoch: 49 | Time: 0m 3s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 5.630 |  Val. PPL: 278.551\n",
            "Epoch: 50 | Time: 0m 3s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 5.638 |  Val. PPL: 280.987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6oyDsDog9Dx"
      },
      "source": [
        "import spacy\r\n",
        "def generate_code(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('en')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "    \r\n",
        "    src_mask = model.make_src_mask(src_tensor)\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iubY-lFhE_5",
        "outputId": "b21371cc-b9bb-418c-c6e5-60287efe390f"
      },
      "source": [
        "Question_text = 'Write a python program to print largest of three numbers\\n '\r\n",
        "\r\n",
        "print(Question_text )\r\n",
        "\r\n",
        "translation, attention = generate_code(Question_text , Program_text, Program_code, model, device)\r\n",
        "\r\n",
        "for i in range(len(translation)):\r\n",
        "  print(end =\" \")\r\n",
        "  print(translation[i])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a python program to print largest of three numbers\n",
            " \n",
            " a = 15\n",
            " b = float(input(\"please enter the first value: \"))\n",
            " c = float(input(\"please enter the first value: \"))\n",
            " if (a > b and a > c):\n",
            "           print(\"{0} is greater than both {1} and {2}\". format(a, b, c))\n",
            " elif (b > a and b > c):\n",
            "           print(\"{0} is greater than both {1} and {2}\". format(b, a, c))\n",
            " elif (c > a and c > b):\n",
            "           print(\"{0} is greater than both {1} and {2}\". format(c, a, b))\n",
            " else:\n",
            "           print(\"either any two values or all the three values are equal\")\n",
            " \n",
            " end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQh5fC6h27So",
        "outputId": "896ecefe-cf7b-4f1f-d378-9ed82ba3c5d3"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "for i in range(25):\r\n",
        "  print(\"++++++++++++++++++++++\\n\\n\\nExample\", i+1)\r\n",
        "  inf = random.randint(1,2500)\r\n",
        "  src = vars(train_data.examples[inf])['Program_text']\r\n",
        "  print(' '.join(src))\r\n",
        "  translation, attention = generate_code(src , Program_text, Program_code, model, device)\r\n",
        "\r\n",
        "  for i in range(len(translation)):\r\n",
        "    print(end =\" \")\r\n",
        "    print(translation[i])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 1\n",
            "  write a python function to check whether a given year is a leap year\n",
            " def leapyear_check(year):\n",
            "     if(year%4==0 and year%100!=0 or year%400==0):\n",
            "         return true\n",
            "     else:\n",
            "         return false\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 2\n",
            "  write a function to merge dictionaries\n",
            " def merge1():\n",
            "     test_list1 = [{\"a\": 1, \"b\": 4}, {\"c\": 10, \"d\": 15},\n",
            "                   {\"f\": \"gfg\"}]\n",
            "     test_list2 = [{\"e\": 6}, {\"f\": 3, \"fg\": 10, \"h\": 1},\n",
            "                   {\"i\": 10}]\n",
            "     print(\"the original list 1 is : \" + str(test_list1))\n",
            "     print(\"the original list 2 is : \" + str(test_list2))\n",
            "     for idx in range(0, len(test_list1)):\n",
            "         id_keys = list(test_list1[idx].keys())\n",
            "         for key in test_list2[idx]:\n",
            "             if key not in id_keys:\n",
            "                 test_list1[idx][key] = test_list2[idx][key]\n",
            "     print(\"the merged dictionary list : \" + str(test_list1))\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 3\n",
            "   write a python function to get the volume of a cone with radius & vertical height as input\n",
            " def cone_volume(radius, height):\n",
            "     volume = 1/3 * 3.14 * (radius ** 2) * height\n",
            "     return volume\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 4\n",
            "  write a program to to check if a triangle is valid or not , given it 's all three angles\n",
            " def is_valid_triangle_angle(a, b c):\n",
            "     if a+b+c == 180:\n",
            "         return true\n",
            "     return false\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 5\n",
            "89 write a python program to flatten tuples list to string and print it\n",
            " test_list = [('1', '4', '6'), ('5', '8'), ('2', '9'), ('1', '10')] \n",
            " print(\"the original list : \" + str(test_list)) \n",
            " s=''\n",
            " for i in test_list:\n",
            "     for j in i:\n",
            "         s+=' '+j+' '\n",
            " print(f' string after flattening is {s}')\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 6\n",
            "  write a function to find out the second maximum number in the given list\n",
            " def find_second_maximum(lst):\n",
            "     max = float('-inf')\n",
            "     sec_max = float('-inf')\n",
            "     for elem in list:\n",
            "         if elem > max:\n",
            "             sec_max = max\n",
            "             max = elem\n",
            "         elif elem > sec_max:\n",
            "             sec_max = elem\n",
            "     return sec_max\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 7\n",
            "4 write a program to print even numbers in a list\n",
            " list1 = [10, 21, 4, 45, 66, 93] \n",
            " for num in list1: \n",
            "     if num % 2 == 0: \n",
            "        print(num, end = \" \") \n",
            "        \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 8\n",
            "47 . python program to sort words in alphabetic order\n",
            " my_str = \"hello this is an example with cased letters\"\n",
            " words = [word.lower() for word in my_str.split()]\n",
            " words.sort()\n",
            " print(\"the sorted words are:\")\n",
            " for word in words:\n",
            "    print(word)\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 9\n",
            "  write a function that will convert a string into camelcase\n",
            " from re import sub\n",
            " def camelcase(string):\n",
            " \tstring = sub(r\"(_|-)+\", \" \", string).title().replace(\" \", \"\")\n",
            " \treturn string[0].lower() + string[1:]\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 10\n",
            "  calculate length of a string\n",
            " word = \"hello world\"\n",
            " print(f\"length of string: {len(word)}\")\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 11\n",
            "  write a python function to return powerset of iterable\n",
            " def powerset(iterable):\n",
            "     \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
            "     from itertools import chain, combinations\n",
            "     s = list(iterable)\n",
            "     return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
            " list(powerset([1,2,3]))\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 12\n",
            "  write a program to compute the count of each word in a sentence and print it\n",
            " word_freq = {}\n",
            " line = 'how many how words does this many have'\n",
            " for word in line.split():\n",
            "    word_freq[word] = word_freq.get(word, 0) + 1\n",
            " print(word_freq)\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 13\n",
            "  write a python function to check if 2 strings are anagrams or not\n",
            " def anagram(s1, s2):\n",
            "     if sorted(s1) == sorted(s2):\n",
            "         return true\n",
            "     else:\n",
            "         return false\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 14\n",
            "  write a python program to print all numbers in a range which are perfect squares and sum of all digits in the number is less than 10\n",
            " l=6\n",
            " u=9\n",
            " a=[x for x in range(l,u+1) if (int(x**0.5))**2==x and sum(list(map(int,str(x))))<10]\n",
            " print(a)\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 15\n",
            "  write a python program to modify the second item ( 33 ) of a list inside a following tuple to 333\n",
            " tuple1 = (11, [22, 33], 44, 55)\n",
            " tuple1[1][1] = 333\n",
            " print(tuple1)\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 16\n",
            "  now it is comma delimited\n",
            "         print(f'{fn.__name__}({args_str}) took {elapsed} seconds')\n",
            "         return result\n",
            "     \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 17\n",
            "  write a python function that takes a sequence of numbers and determines whether all the numbers are different from each other\n",
            " def test_distinct(data):\n",
            "   if len(data) == len(set(data)):\n",
            "     return true\n",
            "   else:\n",
            "     return false\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 18\n",
            "  write a python function to find the compound interest in python when principle amount , rate of interest and time is given\n",
            " def compound_interest(p,r,t):\n",
            "     ci = p * (pow((1 + r / 100), t)) \n",
            "     return ci\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 19\n",
            "  write a python function that returns the weighted average of numbers\n",
            " def get_weighted_average(numbers, weightage):\n",
            "    return sum(x * y for x, y in zip(numbers, weightage)) / sum(weightage)\n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 20\n",
            "  write a python program to implement bubble sort and print the result\n",
            " from random import randint\n",
            " n = 7\n",
            " a = []\n",
            " for i in range(n):\n",
            "     a.append(randint(1, 20))\n",
            " print(a)\n",
            " for i in range(n-1):\n",
            "     for j in range(n-i-1):\n",
            "         if a[j] > a[j+1]:\n",
            "             b = a[j]\n",
            "             a[j] = a[j+1]\n",
            "             a[j+1] = b\n",
            " print(a)\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 21\n",
            "  write a python program to print equal lenght of string\n",
            " print('ab'.zfill(5))\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 22\n",
            "  write a python function to return the second smallest number in a list and print it . example : input second_smallest([1 , 2 , -8 , -2 , 0 ] ) output -2\n",
            " def second_smallest(numbers):\n",
            "     a1, a2 = float('inf'), float('inf')\n",
            "     for x in numbers:\n",
            "         if x <= a1:\n",
            "             a1, a2 = x, a1\n",
            "         elif x < a2:\n",
            "             a2 = x\n",
            "     return a2\n",
            " print(second_smallest([1, 2, -8, -2, 0]))\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 23\n",
            "  write a python program to remove tuples of length 1 from a list of tuples . print the final list .\n",
            "   \n",
            " test_list = [(4, 5), (4, ), (8, 6, 7), (1, ), (3, 4, 6, 7)] \n",
            " k = 1\n",
            " res = [ele for ele in test_list if len(ele) != k] \n",
            "   \n",
            " print(\"filtered list : \" + str(res))\n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 24\n",
            "  write a python function to get the volume of a rectangular prism with side as length , width and height as input\n",
            " def rec_prism_volume(length, width, height):\n",
            "     volume = length * width * height\n",
            "     return volume \n",
            " \n",
            " \n",
            " end\n",
            "++++++++++++++++++++++\n",
            "\n",
            "\n",
            "Example 25\n",
            "61 write a program to print the elements of an array present on even position\n",
            " arr = [1, 2, 3, 4, 5];     \n",
            "      \n",
            " print(\"elements of given array present on even position: \");    \n",
            "     \n",
            " for i in range(1, len(arr), 2):    \n",
            "     print(arr[i]);   \n",
            "     \n",
            " \n",
            " end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb_7_6kSHg3W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}